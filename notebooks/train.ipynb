{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define las transformaciones\n",
    "resize = transforms.Resize((112, 112))\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "rotation = transforms.RandomRotation(degrees=10)\n",
    "color_jitter = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "random_crop = transforms.RandomResizedCrop(size=(112, 112), scale=(0.9, 1.0), ratio=(0.9, 1.1))\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Función para aplicar transformaciones\n",
    "def apply_transform_list(imgs, train=True):\n",
    "    # Seed para asegurarse de que las transformaciones aleatorias sean consistentes\n",
    "    seed = np.random.randint(2147483647)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generar parámetros aleatorios para las transformaciones\n",
    "    params = {\n",
    "        'horizontal_flip': random.random(),\n",
    "        'rotation': random.uniform(-10, 10),\n",
    "        'brightness': random.uniform(0.9, 1.1),\n",
    "        'contrast': random.uniform(0.9, 1.1),\n",
    "        'saturation': random.uniform(0.9, 1.1),\n",
    "        'hue': random.uniform(-0.1, 0.1),\n",
    "        'crop_params': random_crop.get_params(resize(imgs[0]), scale=(0.9, 1.0), ratio=(0.9, 1.1))\n",
    "    }\n",
    "\n",
    "    new_imgs = []\n",
    "\n",
    "    for img in imgs:\n",
    "        img = resize(img)\n",
    "        \n",
    "        if train:\n",
    "            # Aplicar transformaciones solo en entrenamiento\n",
    "            if params['horizontal_flip'] < 0.5:\n",
    "                img = transforms.functional.hflip(img)\n",
    "            img = transforms.functional.rotate(img, params['rotation'])\n",
    "            img = transforms.functional.adjust_brightness(img, params['brightness'])\n",
    "            img = transforms.functional.adjust_contrast(img, params['contrast'])\n",
    "            img = transforms.functional.adjust_saturation(img, params['saturation'])\n",
    "            img = transforms.functional.adjust_hue(img, params['hue'])\n",
    "            img = transforms.functional.resized_crop(img, *params['crop_params'], size=(112, 112))\n",
    "\n",
    "        img = to_tensor(img)\n",
    "        img = normalize(img)\n",
    "\n",
    "        new_imgs.append(img)\n",
    "    \n",
    "    return new_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_transform_list\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFireSeriesDataset\u001b[39;00m(Dataset):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir, img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m112\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'custom_tf'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import random\n",
    "\n",
    "class FireSeriesDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_size=112, transform=None, train=True):\n",
    "        self.transform = transform\n",
    "        self.sets = glob.glob(f\"{root_dir}/**/*\")\n",
    "        self.img_size = img_size\n",
    "        self.train = train  # Para diferenciar entre entrenamiento y validación/prueba\n",
    "        random.shuffle(self.sets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_folder = self.sets[idx]\n",
    "        img_list = glob.glob(f\"{img_folder}/*.jpg\")\n",
    "\n",
    "        # Cargar todas las imágenes disponibles\n",
    "        images = [Image.open(file) for file in img_list]\n",
    "\n",
    "        # Redimensionar todas las imágenes al tamaño img_size\n",
    "        img_list = [im.resize((self.img_size, self.img_size)) for im in images]\n",
    "\n",
    "        # Aplicar las transformaciones a las imágenes (solo si train=True)\n",
    "        tensor_list = apply_transform_list(img_list, self.train)\n",
    "\n",
    "        # Concatenar todas las imágenes en un solo tensor\n",
    "        return torch.cat(tensor_list, dim=0), int(img_folder.split(\"/\")[-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
