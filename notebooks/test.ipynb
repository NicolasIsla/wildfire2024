{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "def NemoProcess(nemo_path, output_path, test_split=0.1):\n",
    "    \"\"\"\n",
    "    Process the Nemo dataset into YOLO format with images and labels structured for train, val, and test sets.\n",
    "\n",
    "    Args:\n",
    "        nemo_path (str): Path to the root of the Nemo dataset.\n",
    "        output_path (str): Path where the YOLO formatted dataset will be stored.\n",
    "        test_split (float): Fraction of the data to use as a test set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Define paths\n",
    "    train_img_path = os.path.join(nemo_path, 'train', 'img')\n",
    "    train_ann_path = os.path.join(nemo_path, 'train', 'ann')\n",
    "    val_img_path = os.path.join(nemo_path, 'val', 'img')\n",
    "    val_ann_path = os.path.join(nemo_path, 'val', 'ann')\n",
    "\n",
    "    # Create YOLO directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_path, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, 'labels', split), exist_ok=True)\n",
    "\n",
    "    # Copy and split the dataset\n",
    "    def process_split(img_path, ann_path, split):\n",
    "        images = glob.glob(os.path.join(img_path, \"*.jpg\"))  # Assuming images are in .jpg format\n",
    "        labels = glob.glob(os.path.join(ann_path, \"*.json\"))\n",
    "\n",
    "        for img_file, label_file in zip(sorted(images), sorted(labels)):\n",
    "            img_name = os.path.basename(img_file)\n",
    "            label_name = os.path.basename(label_file)\n",
    "\n",
    "            shutil.copy(img_file, os.path.join(output_path, 'images', split, img_name))\n",
    "            shutil.copy(label_file, os.path.join(output_path, 'labels', split, label_name))\n",
    "\n",
    "    # Process train and validation sets\n",
    "    process_split(train_img_path, train_ann_path, 'train')\n",
    "    process_split(val_img_path, val_ann_path, 'val')\n",
    "\n",
    "    # Optionally, move a portion of validation to the test set\n",
    "    val_images = os.listdir(os.path.join(output_path, 'images', 'val'))\n",
    "    val_labels = os.listdir(os.path.join(output_path, 'labels', 'val'))\n",
    "    \n",
    "    num_test = int(len(val_images) * test_split)\n",
    "    test_images = val_images[:num_test]\n",
    "    test_labels = val_labels[:num_test]\n",
    "    \n",
    "    for test_img, test_lbl in zip(test_images, test_labels):\n",
    "        shutil.move(os.path.join(output_path, 'images', 'val', test_img), os.path.join(output_path, 'images', 'test', test_img))\n",
    "        shutil.move(os.path.join(output_path, 'labels', 'val', test_lbl), os.path.join(output_path, 'labels', 'test', test_lbl))\n",
    "\n",
    "   \n",
    "    print(f\"YOLO format dataset created at {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2684\n",
      "250\n",
      "YOLO format dataset created at C:\\nico\\wildfire2024\\data\\test\n"
     ]
    }
   ],
   "source": [
    "NemoProcess(\n",
    "    nemo_path=r'C:\\nico\\wildfire2024\\data\\Nemo',\n",
    "    output_path=r'C:\\nico\\wildfire2024\\data\\test',\n",
    "    test_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def process_labels_nemo(json_path, txt_output_path):\n",
    "    \"\"\"\n",
    "    Convert label data from JSON format to YOLO format.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing the label information.\n",
    "        txt_output_path (str): Path to save the converted YOLO format label (.txt) file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    image_width = data['size']['width']\n",
    "    image_height = data['size']['height']\n",
    "\n",
    "    yolo_labels = []\n",
    "\n",
    "    for obj in data['objects']:\n",
    "        if obj['geometryType'] == 'rectangle':\n",
    "            class_id = 0  # Since we only have one class: smoke\n",
    "\n",
    "            # Extract the exterior points\n",
    "            x_min, y_min = obj['points']['exterior'][0]\n",
    "            x_max, y_max = obj['points']['exterior'][1]\n",
    "\n",
    "            # Convert to YOLO format (x_center, y_center, width, height)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "\n",
    "            yolo_labels.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    # Write the labels to a .txt file\n",
    "    with open(txt_output_path, 'w') as txt_file:\n",
    "        for label in yolo_labels:\n",
    "            txt_file.write(label + '\\n')\n",
    "\n",
    "def NemoProcess2(nemo_path, output_path, test_split=0.1):\n",
    "    \"\"\"\n",
    "    Process the Nemo dataset into YOLO format with images and labels structured for train, val, and test sets.\n",
    "\n",
    "    Args:\n",
    "        nemo_path (str): Path to the root of the Nemo dataset.\n",
    "        output_path (str): Path where the YOLO formatted dataset will be stored.\n",
    "        test_split (float): Fraction of the data to use as a test set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Define paths\n",
    "    train_img_path = os.path.join(nemo_path, 'train', 'img')\n",
    "    train_ann_path = os.path.join(nemo_path, 'train', 'ann')\n",
    "    val_img_path = os.path.join(nemo_path, 'val', 'img')\n",
    "    val_ann_path = os.path.join(nemo_path, 'val', 'ann')\n",
    "\n",
    "    # Create YOLO directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_path, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, 'labels', split), exist_ok=True)\n",
    "\n",
    "    # Copy and split the dataset\n",
    "    def process_split(img_path, ann_path, split):\n",
    "        images = glob.glob(os.path.join(img_path, \"*.jpg\"))  # Assuming images are in .jpg format\n",
    "        labels = glob.glob(os.path.join(ann_path, \"*.json\"))\n",
    "\n",
    "        for img_file, json_file in zip(sorted(images), sorted(labels)):\n",
    "            img_name = os.path.basename(img_file)\n",
    "            txt_output_name = img_name.replace('.jpg', '.txt')\n",
    "            txt_output_path = os.path.join(output_path, 'labels', split, txt_output_name)\n",
    "\n",
    "            # Process the JSON label file\n",
    "            process_labels_nemo(json_file, txt_output_path)\n",
    "\n",
    "            shutil.copy(img_file, os.path.join(output_path, 'images', split, img_name))\n",
    "\n",
    "    # Process train and validation sets\n",
    "    process_split(train_img_path, train_ann_path, 'train')\n",
    "    process_split(val_img_path, val_ann_path, 'val')\n",
    "\n",
    "    # Optionally, move a portion of validation to the test set\n",
    "    train_images = os.listdir(os.path.join(output_path, 'images', 'train'))\n",
    "    train_labels = os.listdir(os.path.join(output_path, 'labels', 'train'))\n",
    "    \n",
    "    num_test = int(len(train_images) * test_split)\n",
    "    test_images = train_images[:num_test]\n",
    "    test_labels = train_labels[:num_test]\n",
    "    \n",
    "    for test_img, test_lbl in zip(test_images, test_labels):\n",
    "        shutil.move(os.path.join(output_path, 'images', 'train', test_img), os.path.join(output_path, 'images', 'test', test_img))\n",
    "        shutil.move(os.path.join(output_path, 'labels', 'train', test_lbl), os.path.join(output_path, 'labels', 'test', test_lbl))\n",
    "    # print len of train, val and test\n",
    "    print(f\"Train: {len(os.listdir(os.path.join(output_path, 'images', 'train')))}\")\n",
    "    print(f\"Val: {len(os.listdir(os.path.join(output_path, 'images', 'val')))}\")\n",
    "    print(f\"Test: {len(os.listdir(os.path.join(output_path, 'images', 'test')))}\")\n",
    "    print(f\"YOLO format dataset created at {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2416\n",
      "Val: 250\n",
      "Test: 268\n",
      "YOLO format dataset created at C:\\nico\\wildfire2024\\data\\NemoFixed\n"
     ]
    }
   ],
   "source": [
    "NemoProcess2(\n",
    "    nemo_path=r'C:\\nico\\wildfire2024\\data\\Nemo',\n",
    "    output_path=r'C:\\nico\\wildfire2024\\data\\NemoFixed',\n",
    "    test_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "\n",
    "def process_labels(json_path, txt_output_path):\n",
    "    \"\"\"\n",
    "    Convert label data from JSON format to YOLO format.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing the label information.\n",
    "        txt_output_path (str): Path to save the converted YOLO format label (.txt) file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    image_width = data['size']['width']\n",
    "    image_height = data['size']['height']\n",
    "\n",
    "    yolo_labels = []\n",
    "\n",
    "    for obj in data['objects']:\n",
    "        if obj['geometryType'] == 'rectangle':\n",
    "            class_id = 0  # Since we only have one class: smoke\n",
    "\n",
    "            # Extract the exterior points\n",
    "            x_min, y_min = obj['points']['exterior'][0]\n",
    "            x_max, y_max = obj['points']['exterior'][1]\n",
    "\n",
    "            # Convert to YOLO format (x_center, y_center, width, height)\n",
    "            x_center = (x_min + x_max) / 2 / image_width\n",
    "            y_center = (y_min + y_max) / 2 / image_height\n",
    "            width = (x_max - x_min) / image_width\n",
    "            height = (y_max - y_min) / image_height\n",
    "\n",
    "            yolo_labels.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    # Write the labels to a .txt file\n",
    "    with open(txt_output_path, 'w') as txt_file:\n",
    "        for label in yolo_labels:\n",
    "            txt_file.write(label + '\\n')\n",
    "\n",
    "def FigLibProcess(figlib_path, output_path, val_split=0.1, test_split=0.1):\n",
    "    \"\"\"\n",
    "    Process the FigLib dataset into YOLO format with images and labels structured for train, val, and test sets.\n",
    "\n",
    "    Args:\n",
    "        figlib_path (str): Path to the root of the FigLib dataset.\n",
    "        output_path (str): Path where the YOLO formatted dataset will be stored.\n",
    "        val_split (float): Fraction of the data to use as a validation set.\n",
    "        test_split (float): Fraction of the data to use as a test set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create YOLO directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_path, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, 'labels', split), exist_ok=True)\n",
    "\n",
    "    # Gather all image-label pairs across all fire events\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    for event_folder in os.listdir(figlib_path):\n",
    "        event_path = os.path.join(figlib_path, event_folder)\n",
    "        if os.path.isdir(event_path):\n",
    "            img_path = os.path.join(event_path, 'images')\n",
    "            ann_path = os.path.join(event_path, 'labels')\n",
    "\n",
    "            images = glob.glob(os.path.join(img_path, \"*.jpg\"))  # Assuming images are in .jpg format\n",
    "            labels = glob.glob(os.path.join(ann_path, \"*.txt\"))\n",
    "\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    combined = list(zip(all_images, all_labels))\n",
    "    random.shuffle(combined)\n",
    "    all_images[:], all_labels[:] = zip(*combined)\n",
    "\n",
    "    # Split the dataset\n",
    "    num_total = len(all_images)\n",
    "    num_test = int(num_total * test_split)\n",
    "    num_val = int(num_total * val_split)\n",
    "\n",
    "    test_images = all_images[:num_test]\n",
    "    test_labels = all_labels[:num_test]\n",
    "\n",
    "    val_images = all_images[num_test:num_test + num_val]\n",
    "    val_labels = all_labels[num_test:num_test + num_val]\n",
    "\n",
    "    train_images = all_images[num_test + num_val:]\n",
    "    train_labels = all_labels[num_test + num_val:]\n",
    "\n",
    "    def process_split(images, labels, split):\n",
    "        for img_file, json_file in zip(images, labels):\n",
    "            img_name = os.path.basename(img_file)\n",
    "            txt_output_name = img_name.replace('.jpg', '.txt')\n",
    "            txt_output_path = os.path.join(output_path, 'labels', split, txt_output_name)\n",
    "\n",
    "            # Process the JSON label file\n",
    "            process_labels(json_file, txt_output_path)\n",
    "\n",
    "            shutil.copy(img_file, os.path.join(output_path, 'images', split, img_name))\n",
    "\n",
    "    # Process each split\n",
    "    process_split(train_images, train_labels, 'train')\n",
    "    process_split(val_images, val_labels, 'val')\n",
    "    process_split(test_images, test_labels, 'test')\n",
    "\n",
    "    print(f\"YOLO format dataset created at {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "\n",
    "def FigLibProcess(figlib_path, output_path, val_split=0.1, test_split=0.1):\n",
    "    \"\"\"\n",
    "    Process the FigLib dataset into YOLO format with images and labels structured for train, val, and test sets.\n",
    "\n",
    "    Args:\n",
    "        figlib_path (str): Path to the root of the FigLib dataset.\n",
    "        output_path (str): Path where the YOLO formatted dataset will be stored.\n",
    "        val_split (float): Fraction of the data to use as a validation set.\n",
    "        test_split (float): Fraction of the data to use as a test set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create YOLO directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_path, 'images', split), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, 'labels', split), exist_ok=True)\n",
    "\n",
    "    # Gather all image-label pairs across all fire events\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "\n",
    "    for event_folder in os.listdir(figlib_path):\n",
    "        event_path = os.path.join(figlib_path, event_folder)\n",
    "        if os.path.isdir(event_path):\n",
    "            # img_path = os.path.join(event_path, 'images')\n",
    "            ann_path = os.path.join(event_path, 'labels')\n",
    "\n",
    "            images = glob.glob(os.path.join(event_path, \"*.jpg\"))  # Assuming images are in .jpg format\n",
    "            labels = glob.glob(os.path.join(ann_path, \"*.txt\"))  # Assuming labels are in .txt format\n",
    "\n",
    "            all_images.extend(images)\n",
    "            all_labels.extend(labels)\n",
    "    print(f\"Total images: {len(all_images)}\")\n",
    "    print(f\"Total labels: {len(all_labels)}\")\n",
    "    # Shuffle the dataset\n",
    "    combined = list(zip(all_images, all_labels))\n",
    "    random.shuffle(combined)\n",
    "    all_images[:], all_labels[:] = zip(*combined)\n",
    "\n",
    "    # Split the dataset\n",
    "    num_total = len(all_images)\n",
    "    num_test = int(num_total * test_split)\n",
    "    num_val = int(num_total * val_split)\n",
    "\n",
    "    test_images = all_images[:num_test]\n",
    "    test_labels = all_labels[:num_test]\n",
    "\n",
    "    val_images = all_images[num_test:num_test + num_val]\n",
    "    val_labels = all_labels[num_test:num_test + num_val]\n",
    "\n",
    "    train_images = all_images[num_test + num_val:]\n",
    "    train_labels = all_labels[num_test + num_val:]\n",
    "\n",
    "    def process_split(images, labels, split):\n",
    "        for img_file, label_file in zip(images, labels):\n",
    "            img_name = os.path.basename(img_file)\n",
    "            label_name = os.path.basename(label_file)\n",
    "\n",
    "            shutil.copy(img_file, os.path.join(output_path, 'images', split, img_name))\n",
    "            shutil.copy(label_file, os.path.join(output_path, 'labels', split, label_name))\n",
    "\n",
    "    # Process each split\n",
    "    process_split(train_images, train_labels, 'train')\n",
    "    process_split(val_images, val_labels, 'val')\n",
    "    process_split(test_images, test_labels, 'test')\n",
    "\n",
    "    print(f\"YOLO format dataset created at {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 24758\n",
      "Total labels: 24758\n",
      "YOLO format dataset created at C:\\nico\\wildfire2024\\data\\testfig\n"
     ]
    }
   ],
   "source": [
    "figlib_path = r\"C:\\nico\\wildfire2024\\data\\Fig_Lib\\FIGLIB_ANNOTATED_RESIZED\"  # Path to the root of the extracted FigLib dataset\n",
    "output_path = r\"C:\\nico\\wildfire2024\\data\\testfig\"  # Path where you want to store the YOLO formatted dataset\n",
    "\n",
    "FigLibProcess(figlib_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS-71c1fd51-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_test_split_with_copy(dataset_path, new_dataset_path, test_split=0.1):\n",
    "    \"\"\"\n",
    "    Create a test split by copying the entire dataset to a new location,\n",
    "    then moving a portion of the train set to a new test set. Also, create\n",
    "    empty .txt files for images that lack corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the original dataset with 'train' and 'val' directories.\n",
    "        new_dataset_path (str): Path to the new dataset location.\n",
    "        test_split (float): Fraction of the train set to use as a test set.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the entire dataset to a new location\n",
    "    if os.path.exists(new_dataset_path):\n",
    "        print(f\"Removing existing directory at {new_dataset_path}\")\n",
    "        shutil.rmtree(new_dataset_path)\n",
    "    shutil.copytree(dataset_path, new_dataset_path)\n",
    "    print(f\"Copied dataset from {dataset_path} to {new_dataset_path}\")\n",
    "\n",
    "    # Define paths in the new dataset\n",
    "    train_img_path = os.path.join(new_dataset_path, 'images', 'train')\n",
    "    train_label_path = os.path.join(new_dataset_path, 'labels', 'train')\n",
    "\n",
    "    val_img_path = os.path.join(new_dataset_path, 'images', 'val')\n",
    "    val_label_path = os.path.join(new_dataset_path, 'labels', 'val')\n",
    "\n",
    "    test_img_path = os.path.join(new_dataset_path, 'images', 'test')\n",
    "    test_label_path = os.path.join(new_dataset_path, 'labels', 'test')\n",
    "\n",
    "    # Create test directories if they don't exist\n",
    "    os.makedirs(test_img_path, exist_ok=True)\n",
    "    os.makedirs(test_label_path, exist_ok=True)\n",
    "\n",
    "    # Ensure all images have corresponding labels\n",
    "    def ensure_labels_exist(image_dir, label_dir):\n",
    "        images = sorted(os.listdir(image_dir))\n",
    "        labels = set(os.listdir(label_dir))\n",
    "\n",
    "        for img_name in images:\n",
    "            label_name = img_name.replace('.jpg', '.txt')  # Assuming the images are in .jpg format\n",
    "            if label_name not in labels:\n",
    "                # Create an empty label file if it doesn't exist\n",
    "                open(os.path.join(label_dir, label_name), 'w').close()\n",
    "\n",
    "    # Ensure labels exist in train and val sets\n",
    "    ensure_labels_exist(train_img_path, train_label_path)\n",
    "    ensure_labels_exist(val_img_path, val_label_path)\n",
    "\n",
    "    # List all images and corresponding labels\n",
    "    train_images = sorted(os.listdir(train_img_path))\n",
    "    train_labels = sorted(os.listdir(train_label_path))\n",
    "\n",
    "    # Shuffle and split the dataset\n",
    "    combined = list(zip(train_images, train_labels))\n",
    "    random.shuffle(combined)\n",
    "    train_images[:], train_labels[:] = zip(*combined)\n",
    "\n",
    "    num_test = int(len(train_images) * test_split)\n",
    "\n",
    "    test_images = train_images[:num_test]\n",
    "    test_labels = train_labels[:num_test]\n",
    "\n",
    "    # Move the selected images and labels to the test set\n",
    "    for img_name, label_name in zip(test_images, test_labels):\n",
    "        shutil.move(os.path.join(train_img_path, img_name), os.path.join(test_img_path, img_name))\n",
    "        shutil.move(os.path.join(train_label_path, label_name), os.path.join(test_label_path, label_name))\n",
    "\n",
    "    print(f\"Moved {num_test} images and labels to the test set.\")\n",
    "\n",
    "    # Ensure labels exist in the test set\n",
    "    ensure_labels_exist(test_img_path, test_label_path)\n",
    "\n",
    "    print(f\"Completed creating the test set and ensuring all images have labels.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied dataset from C:\\nico\\wildfire2024\\data\\DS-71c1fd51-v2 to C:\\nico\\wildfire2024\\data\\DS-71c1fd51-v2-Fixed\n",
      "Moved 670 images and labels to the test set.\n",
      "Completed creating the test set and ensuring all images have labels.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\nico\\wildfire2024\\data\\DS-71c1fd51-v2\"  # Path to the original dataset\n",
    "new_dataset_path = r\"C:\\nico\\wildfire2024\\data\\DS-71c1fd51-v2-Fixed\"  # Path to the new dataset copy\n",
    "\n",
    "create_test_split_with_copy(dataset_path, new_dataset_path, test_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pyronear_marguerite_2_2023_08_28T05_41_51.jpg', 'pyronear_brison_4_2023_11_02T10_55_20.jpg', 'pyronear_courmettes_1_2023_11_07T06_31_00.jpg', 'pyronear_salaunes_1_4_2023_08_30T08_08_30.jpg', 'AWF_axis-bryantmtn1_2023_06_04T06_54_58.jpg', 'pyronear_courmettes_4_2023_11_06T14_43_01.jpg', 'awf_nvseismolab_noaaX00101_2019_07_05T13_46_05.jpg', 'pyronear_salaunes_1_1_2023_09_02T18_44_37.jpg', 'AWF_axis-mtharrison_2023_06_04T18_20_03.jpg', 'pyronear_marguerite_4_2023_10_25T08_29_52.jpg', 'pyronear_valbonne_3_2023_11_02T16_34_18.jpg', 'pyronear_cabanelle_3_2023_10_14T17_06_48.jpg', 'pyronear_cabanelle_3_2023_10_05T16_37_42.jpg', 'pyronear_marguerite_1_2023_10_30T12_49_32.jpg', 'pyronear_ferion_4_2023_10_29T15_07_44.jpg', 'pyronear_courmettes_3_2023_10_21T06_23_31.jpg', 'pyronear_brison_3_2023_09_09T07_04_13.jpg', 'pyronear_brison_3_2023_10_28T11_40_49.jpg', 'Pyronear_brison_3_2023_06_06T05_04_26.jpg', 'AWF_axis-ellamtn2_2023_06_04T05_00_25.jpg', 'AWF_axis-riddle_2023_06_04T09_56_18.jpg', 'pyronear_st_peray_1_2023_09_01T14_17_32.jpg', 'AWF_axis-tahoedonner_2023_06_02T06_46_57.jpg', 'pyronear_marguerite_1_2023_09_12T17_58_18.jpg', 'Pyronear_st_peray_1_2023_05_28T08_17_15.jpg', 'pyronear_st_peray_1_2023_08_24T12_52_27.jpg', 'awf_nvseismolab_mcclellanX00180_2017_09_28T17_22_00.jpg', 'pyronear_marguerite_4_2023_09_21T11_35_24.jpg', 'pyronear_cabanelle_3_2023_09_30T17_35_09.jpg', 'pyronear_marguerite_1_2023_09_14T11_30_36.jpg', 'pyronear_marguerite_1_2023_10_11T15_35_05.jpg', 'pyronear_valbonne_2_2023_10_03T14_13_30.jpg', 'pyronear_brison_2_2023_07_28T07_09_54.jpg', 'pyronear_marguerite_1_2023_08_19T09_14_52.jpg', 'pyronear_marguerite_1_2023_08_12T14_25_41.jpg', 'pyronear_courmettes_4_2023_10_18T15_55_03.jpg', 'pyronear_brison_4_2023_10_14T16_40_34.jpg', 'pyronear_valbonne_4_2023_11_02T08_56_36.jpg', 'pyronear_courmettes_4_2023_10_13T10_19_24.jpg', 'pyronear_cabanelle_1_2023_10_18T15_13_31.jpg', 'pyronear_brison_4_2023_08_02T18_54_01.jpg', 'AWF_axis-winters1_2023_06_01T20_23_22.jpg', 'pyronear_brison_1_2023_11_02T08_57_10.jpg', 'pyronear_courmettes_1_2023_10_29T07_00_10.jpg', 'pyronear_marguerite_2_2023_08_15T18_44_50.jpg', 'pyronear_valbonne_3_2023_11_06T15_55_21.jpg', 'pyronear_marguerite_4_2023_06_10T12_29_14.jpg', 'awf_nvseismolab_bighillX00153_2016_08_24T11_46_27.jpg', 'pyronear_brison_1_2023_08_02T04_55_21.jpg', 'pyronear_courmettes_3_2023_10_14T14_06_50.jpg', 'pyronear_marguerite_2_2023_10_12T10_34_14.jpg', 'AWF_axis-bryantmtn1_2023_06_02T06_43_47.jpg', 'pyronear_courmettes_3_2023_10_20T16_54_56.jpg', 'pyronear_salaunes_2_3_2023_09_02T15_18_55.jpg', 'AWF_axis-mtharrison_2023_06_04T19_49_03.jpg', 'pyronear_brison_1_2023_07_26T10_42_26.jpg', 'pyronear_cabanelle_3_2023_11_07T13_36_04.jpg', 'pyronear_brison_3_2023_08_04T05_02_26.jpg', 'pyronear_brison_3_2023_08_15T16_29_59.jpg', 'pyronear_salaunes_2_1_2023_10_10T07_35_27.jpg', 'pyronear_valbonne_3_2023_10_30T16_28_00.jpg', 'pyronear_marguerite_3_2023_08_13T13_25_13.jpg', 'pyronear_st_peray_2_2023_08_06T05_15_08.jpg', 'pyronear_marguerite_1_2023_10_14T15_33_36.jpg', 'pyronear_valbonne_4_2023_09_30T07_32_18.jpg', 'pyronear_brison_4_2023_10_26T17_01_28.jpg', 'pyronear_marguerite_2_2023_08_26T09_00_02.jpg', 'pyronear_salaunes_2_1_2023_09_12T16_32_21.jpg', 'pyronear_marguerite_3_2023_11_03T15_38_05.jpg', 'pyronear_st_peray_2_2023_08_13T04_54_45.jpg', 'pyronear_marguerite_4_2023_10_24T11_13_48.jpg', 'pyronear_adf_1340_2023_06_17T22_03_50.jpg', 'pyronear_marguerite_1_2023_08_17T07_31_37.jpg', 'AWF_axis-aeneas_2023_06_01T05_32_26.jpg', 'pyronear_marguerite_3_2023_09_13T05_08_32.jpg', 'pyronear_salaunes_2_1_2023_09_20T15_57_46.jpg', 'AWF_axis-farnsworth_2023_06_01T07_18_57.jpg', 'AWF_axis-steamboat_2023_06_01T14_49_54.jpg', 'pyronear_salaunes_1_1_2023_08_14T09_07_24.jpg', 'AWF_axis-northmok_2023_06_01T20_19_51.jpg', 'pyronear_brison_4_2023_07_12T03_49_42.jpg', 'pyronear_marguerite_3_2023_10_25T12_02_34.jpg', 'pyronear_brison_3_2023_10_30T15_46_31.jpg', 'pyronear_st_peray_1_2023_07_26T07_35_39.jpg', 'AWF_axis-bendodot_2023_06_03T06_38_08.jpg', 'AWF_axis-wagontire_2023_06_01T19_59_57.jpg', 'AWF_axis-piercemtn2_2023_06_02T20_12_04.jpg', 'pyronear_brison_2_2023_07_14T06_35_11.jpg', 'pyronear_st_peray_2_2023_06_29T16_24_59.jpg', 'pyronear_courmettes_4_2023_10_25T11_37_18.jpg', 'pyronear_brison_3_2023_10_29T09_40_38.jpg', 'AWF_axis-riddle_2023_06_02T19_25_06.jpg', 'pyronear_marguerite_1_2023_08_23T07_37_59.jpg', 'AWF_axis-baldyjacksonor_2023_06_03T16_35_13.jpg', 'AWF_axis-lakemountain_2023_06_03T07_28_42.jpg', 'pyronear_courmettes_4_2023_10_13T06_34_24.jpg', 'AWF_axis-virginia_2023_06_01T15_50_15.jpg', 'pyronear_st_peray_2_2023_07_26T19_12_42.jpg', 'pyronear_brison_4_2023_07_09T19_01_42.jpg', 'AWF_axis-wagontire_2023_06_01T07_53_07.jpg', 'pyronear_brison_4_2023_06_29T17_23_58.jpg', 'pyronear_cabanelle_2_2023_10_13T15_00_18.jpg', 'pyronear_salaunes_2_1_2023_10_11T08_04_26.jpg', 'AWF_axis-longmtnjacksonor_2023_06_01T06_23_05.jpg', 'AWF_axis-delano_2023_06_03T06_17_15.jpg', 'pyronear_ferion_3_2023_11_07T09_41_13.jpg', 'pyronear_cabanelle_2_2023_09_30T06_06_11.jpg', 'AWF_axis-biglookout_2023_06_02T11_17_47.jpg', 'Pyronear_st_peray_1_2023_05_13T07_55_03.jpg', 'pyronear_marguerite_2_2023_08_13T14_19_12.jpg', 'pyronear_courmettes_3_2023_10_26T15_18_12.jpg', 'pyronear_adf_1340_2023_07_20T11_46_44.jpg', 'pyronear_brison_3_2023_06_13T15_40_37.jpg', 'AWF_axis-hillside_2023_06_01T18_32_17.jpg', 'pyronear_courmettes_3_2023_10_29T08_57_17.jpg', 'AWF_axis-wallsburg_2023_06_01T20_48_07.jpg', 'Pyronear_brison_3_2023_06_04T16_14_32.jpg', 'pyronear_brison_1_2023_10_25T18_28_25.jpg', 'pyronear_salaunes_2_1_2023_08_11T04_49_34.jpg', 'pyronear_marguerite_1_2023_08_13T09_39_30.jpg', 'pyronear_valbonne_3_2023_10_11T17_23_55.jpg', 'AWF_axis-uoregon2_2023_06_03T17_58_51.jpg', 'AWF_axis-bryantmtn2_2023_06_01T19_12_11.jpg', 'pyronear_brison_3_2023_10_30T18_15_17.jpg', 'pyronear_cabanelle_3_2023_09_28T15_11_01.jpg', 'AWF_axis-lewis_2023_06_03T18_01_43.jpg', 'pyronear_marguerite_1_2023_07_05T05_05_26.jpg', 'pyronear_brison_2_2023_07_10T05_23_17.jpg', 'AWF_axis-cave2_2023_06_02T19_25_41.jpg', 'pyronear_valbonne_1_2023_10_28T16_17_56.jpg', 'pyronear_st_peray_2_2023_08_03T19_43_52.jpg', 'pyronear_brison_3_2023_07_24T06_32_07.jpg', 'pyronear_marguerite_1_2023_09_13T16_34_19.jpg', 'pyronear_cabanelle_3_2023_11_01T09_00_23.jpg', 'AWF_axis-bryantmtn1_2023_06_03T17_02_25.jpg', 'pyronear_salaunes_2_3_2023_09_22T17_26_51.jpg', 'pyronear_brison_4_2023_08_22T15_08_30.jpg', 'pyronear_valbonne_1_2023_11_05T06_22_45.jpg', 'pyronear_marguerite_1_2023_08_05T05_22_31.jpg', 'AWF_axis-martis_2023_06_02T20_06_43.jpg', 'Pyronear_marguerite_4_2023_05_13T11_31_16.jpg', 'pyronear_marguerite_4_2023_06_22T12_34_15.jpg', 'pyronear_brison_1_2023_07_14T09_35_43.jpg', 'pyronear_marguerite_3_2023_10_03T05_43_53.jpg', 'AWF_axis-uoregon2_2023_06_02T19_32_25.jpg', 'pyronear_brison_4_2023_10_17T16_37_12.jpg', 'pyronear_valbonne_1_2023_11_06T16_08_45.jpg', 'pyronear_salaunes_1_3_2023_07_14T19_15_44.jpg', 'pyronear_brison_3_2023_10_17T09_35_45.jpg', 'pyronear_cabanelle_2_2023_10_11T08_52_04.jpg', 'pyronear_brison_4_2023_07_25T04_48_41.jpg', 'pyronear_st_peray_2_2023_06_13T07_24_56.jpg', 'pyronear_cabanelle_4_2023_09_30T17_33_41.jpg', 'pyronear_brison_1_2023_08_05T04_54_12.jpg', 'AWF_axis-rockland_2023_06_01T06_50_56.jpg', 'pyronear_cabanelle_2_2023_10_14T09_40_28.jpg', 'pyronear_salaunes_2_1_2023_10_21T06_08_58.jpg', 'AWF_axis-biglookout_2023_06_03T18_41_21.jpg', 'AWF_axis-biglookout_2023_06_04T12_00_14.jpg', 'pyronear_ferion_4_2023_11_07T16_32_57.jpg', 'pyronear_salaunes_1_4_2023_07_18T20_02_46.jpg', 'pyronear_brison_3_2023_09_23T16_18_36.jpg', 'pyronear_valbonne_2_2023_10_01T15_16_29.jpg', 'pyronear_valbonne_1_2023_10_16T07_17_00.jpg', 'pyronear_brison_2_2023_06_13T19_02_56.jpg', 'pyronear_marguerite_2_2023_11_03T11_04_31.jpg', 'pyronear_salaunes_1_3_2023_10_21T06_48_01.jpg', 'pyronear_marguerite_3_2023_06_30T05_27_29.jpg', 'pyronear_brison_3_2023_09_22T05_27_10.jpg', 'pyronear_ferion_3_2023_11_04T15_37_12.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Rutas a las carpetas que contienen los archivos txt y jpg\n",
    "carpeta_txt = r'C:\\Users\\corpa\\Downloads\\pyronear_ds_03_2024_extraido\\pyronear_ds_03_2024\\labels\\val'\n",
    "carpeta_jpg = r'C:\\Users\\corpa\\Downloads\\pyronear_ds_03_2024_extraido\\pyronear_ds_03_2024\\images\\val'\n",
    "\n",
    "# Crear conjuntos con los nombres de archivo sin la extensi√≥n\n",
    "nombres_txt = {archivo.split('.')[0] for archivo in os.listdir(carpeta_txt) if archivo.endswith('.txt')}\n",
    "nombres_jpg = {archivo.split('.')[0] for archivo in os.listdir(carpeta_jpg) if archivo.endswith('.jpg')}\n",
    "\n",
    "# Encontrar nombres de jpg que no tienen un correspondiente archivo txt\n",
    "jpg_sin_txt = nombres_jpg - nombres_txt\n",
    "\n",
    "# Crear una lista de los nombres de archivos jpg que no tienen txt correspondiente\n",
    "archivos_jpg_sin_txt = [nombre + '.jpg' for nombre in jpg_sin_txt]\n",
    "\n",
    "print(archivos_jpg_sin_txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nombres_jpg)- len(nombres_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
