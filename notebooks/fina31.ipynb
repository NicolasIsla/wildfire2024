{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the transformations\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "resize = transforms.Resize((112, 112))\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "rotation = transforms.RandomRotation(degrees=10)\n",
    "color_jitter = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "random_crop = transforms.RandomResizedCrop(size=(112, 112), scale=(0.9, 1.0), ratio=(0.9, 1.1))\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "# Function to apply the transformations using the generated parameters\n",
    "def apply_transform_list(imgs, is_train=True):\n",
    "    # Seed the random number generators\n",
    "    seed = np.random.randint(2147483647)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate random transformation parameters\n",
    "    params = {\n",
    "        'horizontal_flip': random.random(),\n",
    "        'rotation': random.uniform(-10, 10),\n",
    "        'brightness': random.uniform(0.9, 1.1),\n",
    "        'contrast': random.uniform(0.9, 1.1),\n",
    "        'saturation': random.uniform(0.9, 1.1),\n",
    "        'hue': random.uniform(-0.1, 0.1),\n",
    "        'crop_params': random_crop.get_params(resize(imgs[0]), scale=(0.9, 1.0), ratio=(0.9, 1.1))\n",
    "    }\n",
    "\n",
    "    new_imgs = []\n",
    "\n",
    "    for img in imgs:\n",
    "        img = resize(img)\n",
    "        if is_train:\n",
    "            if params['horizontal_flip'] < 0.5:\n",
    "                img = transforms.functional.hflip(img)\n",
    "\n",
    "            img = transforms.functional.rotate(img, params['rotation'])\n",
    "\n",
    "            img = transforms.functional.adjust_brightness(img, params['brightness'])\n",
    "            img = transforms.functional.adjust_contrast(img, params['contrast'])\n",
    "            img = transforms.functional.adjust_saturation(img, params['saturation'])\n",
    "            img = transforms.functional.adjust_hue(img, params['hue'])\n",
    "            img = transforms.functional.resized_crop(img, *params['crop_params'], size=(112, 112))\n",
    "        img = to_tensor(img)\n",
    "        img = normalize(img)\n",
    "\n",
    "        new_imgs.append(img)\n",
    "\n",
    "    return new_imgs\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "\n",
    "class FireSeriesDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_size=112, transform=None, is_train=True):\n",
    "        self.transform = transform\n",
    "        self.sets = glob.glob(f\"{root_dir}/**/*\")\n",
    "        self.img_size=img_size\n",
    "        self.is_train = is_train\n",
    "        random.shuffle(self.sets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_folder = self.sets[idx]\n",
    "        img_list = glob.glob(f\"{img_folder}/*.jpg\")\n",
    "\n",
    "        labels = []\n",
    "        for file in img_list:\n",
    "            label_file = file.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "            with open(label_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            labels.append(np.array(lines[0].split(\" \")[1:5]).astype(\"float\"))\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        xc = np.median(labels[:, 0])\n",
    "        yc = np.median(labels[:, 1])\n",
    "        wb = np.max(labels[:, 2])\n",
    "        hb = np.max(labels[:, 3])\n",
    "\n",
    "        # Load all images first\n",
    "        images = [Image.open(file) for file in img_list]\n",
    "        w, h = images[0].size\n",
    "\n",
    "        crop_size = max(wb*h, hb*h)\n",
    "        if crop_size < self.img_size:\n",
    "            crop_size = self.img_size\n",
    "\n",
    "        x0 = int(xc * w - crop_size / 2)\n",
    "        y0 = int(yc * h - crop_size / 2)\n",
    "        x1 = int(xc * w  + crop_size / 2)\n",
    "        y1 = int(yc * h + crop_size / 2)\n",
    "\n",
    "        img_list = []\n",
    "\n",
    "        for im in images:\n",
    "            cropped_image = im.crop(\n",
    "                (x0, y0, x1,y1))\n",
    "\n",
    "            cropped_image = cropped_image.resize((self.img_size, self.img_size))\n",
    "            img_list.append(cropped_image)\n",
    "\n",
    "        tensor_list = apply_transform_list(img_list, is_train=self.is_train)\n",
    "        # txc, w,h to t, x,c,w,h\n",
    "        tensor_list = [tensor.unsqueeze(0) for tensor in tensor_list]\n",
    "\n",
    "        # Concatena a lo largo de una nueva dimensión al principio, formando un tensor de (T, C, W, H)\n",
    "        combined_tensor = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return torch.cat(tensor_list), int(img_folder.split(\"/\")[-2])\n",
    "\n",
    "\n",
    "class FireDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=16, img_size=112, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = FireSeriesDataset(\n",
    "            os.path.join(self.data_dir, \"train\"), self.img_size, is_train=True\n",
    "        )\n",
    "        self.val_dataset = FireSeriesDataset(\n",
    "            os.path.join(self.data_dir, \"val\"), self.img_size, is_train=False\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "class FireClassifier(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4):\n",
    "        super(FireClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Usamos una ResNet como extractor de características.\n",
    "        # Pretrained sobre ImageNet, usualmente se carga con 3 canales.\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Removemos la capa final para usarla como extractor de características.\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "\n",
    "        # LSTM que procesará las características extraídas.\n",
    "        # Número de características de la salida del último bloque conv de ResNet.\n",
    "        num_features = resnet.fc.in_features\n",
    "        self.lstm = nn.LSTM(input_size=32768, hidden_size=256, batch_first=True, num_layers=1)\n",
    "\n",
    "        # Capa de clasificación.\n",
    "        self.classifier = nn.Linear(256, 1)  # Salida binaria\n",
    "\n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Métricas\n",
    "        self.train_accuracy = Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = Accuracy(task=\"binary\")\n",
    "        self.train_precision = Precision(task=\"binary\")\n",
    "        self.val_precision = Precision(task=\"binary\")\n",
    "        self.train_recall = Recall(task=\"binary\")\n",
    "        self.val_recall = Recall(task=\"binary\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, channels, height, width]\n",
    "        # Procesa cada imagen de la secuencia a través del extractor de características.\n",
    "        batch_size, seq_length, C, H, W = x.size()\n",
    "        x = x.view(batch_size * seq_length, C, H, W)\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        # Reformatear salida para la LSTM\n",
    "        # Deberás asegurarte que las dimensiones coincidan con lo que espera la LSTM.\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "\n",
    "        # Pasar las características por la LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "\n",
    "        # Solo nos interesa la última salida de la secuencia para la clasificación\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_hat = self(x).squeeze()\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        acc = self.train_accuracy(torch.sigmoid(y_hat), y.int())\n",
    "        precision = self.train_precision(torch.sigmoid(y_hat), y.int())\n",
    "        recall = self.train_recall(torch.sigmoid(y_hat), y.int())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        self.log(\"train_precision\", precision)\n",
    "        self.log(\"train_recall\", recall)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        acc = self.val_accuracy(torch.sigmoid(y_hat), y.int())\n",
    "        precision = self.val_precision(torch.sigmoid(y_hat), y.int())\n",
    "        recall = self.val_recall(torch.sigmoid(y_hat), y.int())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_precision\", precision)\n",
    "        self.log(\"val_recall\", recall)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams['learning_rate'], weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler\n",
    "        }\n",
    "\n",
    "# # Initialize the DataModule\n",
    "# data_dir = \"/data/nisla/temporal_ds/images\"\n",
    "# data_module = FireDataModule(data_dir)\n",
    "# # set\n",
    "# data_module.setup()\n",
    "# # Función para obtener la imagen y etiqueta por índice de lote y posición\n",
    "# def get_image_by_index(loader, batch_index, img_index):\n",
    "#     for i, (x, y) in enumerate(loader):\n",
    "#         print(x.shape)\n",
    "#         print(y.shape)\n",
    "\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# # Extrae la imagen y etiqueta específicas\n",
    "# image, label = get_image_by_index(train_loader, 1, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\nico\\wildfire2024\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\nico\\wildfire2024\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FireClassifier(\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(32768, 256, batch_first=True)\n",
       "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (train_accuracy): BinaryAccuracy()\n",
       "  (val_accuracy): BinaryAccuracy()\n",
       "  (train_precision): BinaryPrecision()\n",
       "  (val_precision): BinaryPrecision()\n",
       "  (train_recall): BinaryRecall()\n",
       "  (val_recall): BinaryRecall()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FireClassifier()\n",
    "# load the model\n",
    "path = r\"C:\\nico\\model-lstm-resnet.ckpt\"\n",
    "model = FireClassifier.load_from_checkpoint(path)\n",
    "model.eval().cpu()\n",
    "# run prediction\n",
    "# print(images.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 112, 112, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got PngImageFile",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(images))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# dar vuelta 1,4, 3, 112, 112\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(images))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got PngImageFile"
     ]
    }
   ],
   "source": [
    "path_images = r\"C:\\nico\\Nueva carpeta\"\n",
    "# unir las imagenes en una lista\n",
    "image_paths = [os.path.join(path_images, img) for img in os.listdir(path_images)]\n",
    "print(np.shape(image_paths))\n",
    "images = [Image.open(img_path) for img_path in image_paths]\n",
    "print(np.shape(images))\n",
    "# dar vuelta 1,4, 3, 112, 112\n",
    "images = torch.stack(images)\n",
    "images = images.permute(1, 4, 3, 112, 112)\n",
    "print(np.shape(images))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.PngImagePlugin.PngImageFile image mode=RGB size=112x112>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGB size=112x112>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGB size=112x112>,\n",
       " <PIL.PngImagePlugin.PngImageFile image mode=RGB size=112x112>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener la imagen y etiqueta por índice de lote y posición\n",
    "def get_image_by_index(loader, batch_index, img_index):\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "\n",
    "# Extrae la imagen y etiqueta específicas\n",
    "image, label = get_image_by_index(train_loader, 1, 4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
