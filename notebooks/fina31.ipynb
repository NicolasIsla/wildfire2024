{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the transformations\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "resize = transforms.Resize((112, 112))\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "rotation = transforms.RandomRotation(degrees=10)\n",
    "color_jitter = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "random_crop = transforms.RandomResizedCrop(size=(112, 112), scale=(0.9, 1.0), ratio=(0.9, 1.1))\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "# Function to apply the transformations using the generated parameters\n",
    "def apply_transform_list(imgs, is_train=True):\n",
    "    # Seed the random number generators\n",
    "    seed = np.random.randint(2147483647)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Generate random transformation parameters\n",
    "    params = {\n",
    "        'horizontal_flip': random.random(),\n",
    "        'rotation': random.uniform(-10, 10),\n",
    "        'brightness': random.uniform(0.9, 1.1),\n",
    "        'contrast': random.uniform(0.9, 1.1),\n",
    "        'saturation': random.uniform(0.9, 1.1),\n",
    "        'hue': random.uniform(-0.1, 0.1),\n",
    "        'crop_params': random_crop.get_params(resize(imgs[0]), scale=(0.9, 1.0), ratio=(0.9, 1.1))\n",
    "    }\n",
    "\n",
    "    new_imgs = []\n",
    "\n",
    "    for img in imgs:\n",
    "        img = resize(img)\n",
    "        if is_train:\n",
    "            if params['horizontal_flip'] < 0.5:\n",
    "                img = transforms.functional.hflip(img)\n",
    "\n",
    "            img = transforms.functional.rotate(img, params['rotation'])\n",
    "\n",
    "            img = transforms.functional.adjust_brightness(img, params['brightness'])\n",
    "            img = transforms.functional.adjust_contrast(img, params['contrast'])\n",
    "            img = transforms.functional.adjust_saturation(img, params['saturation'])\n",
    "            img = transforms.functional.adjust_hue(img, params['hue'])\n",
    "            img = transforms.functional.resized_crop(img, *params['crop_params'], size=(112, 112))\n",
    "        img = to_tensor(img)\n",
    "        img = normalize(img)\n",
    "\n",
    "        new_imgs.append(img)\n",
    "\n",
    "    return new_imgs\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "\n",
    "class FireSeriesDataset(Dataset):\n",
    "    def __init__(self, root_dir, img_size=112, transform=None, is_train=True):\n",
    "        self.transform = transform\n",
    "        self.sets = glob.glob(f\"{root_dir}/**/*\")\n",
    "        self.img_size=img_size\n",
    "        self.is_train = is_train\n",
    "        random.shuffle(self.sets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_folder = self.sets[idx]\n",
    "        img_list = glob.glob(f\"{img_folder}/*.jpg\")\n",
    "\n",
    "        labels = []\n",
    "        for file in img_list:\n",
    "            label_file = file.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "            with open(label_file, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            labels.append(np.array(lines[0].split(\" \")[1:5]).astype(\"float\"))\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        xc = np.median(labels[:, 0])\n",
    "        yc = np.median(labels[:, 1])\n",
    "        wb = np.max(labels[:, 2])\n",
    "        hb = np.max(labels[:, 3])\n",
    "\n",
    "        # Load all images first\n",
    "        images = [Image.open(file) for file in img_list]\n",
    "        w, h = images[0].size\n",
    "\n",
    "        crop_size = max(wb*h, hb*h)\n",
    "        if crop_size < self.img_size:\n",
    "            crop_size = self.img_size\n",
    "\n",
    "        x0 = int(xc * w - crop_size / 2)\n",
    "        y0 = int(yc * h - crop_size / 2)\n",
    "        x1 = int(xc * w  + crop_size / 2)\n",
    "        y1 = int(yc * h + crop_size / 2)\n",
    "\n",
    "        img_list = []\n",
    "\n",
    "        for im in images:\n",
    "            cropped_image = im.crop(\n",
    "                (x0, y0, x1,y1))\n",
    "\n",
    "            cropped_image = cropped_image.resize((self.img_size, self.img_size))\n",
    "            img_list.append(cropped_image)\n",
    "\n",
    "        tensor_list = apply_transform_list(img_list, is_train=self.is_train)\n",
    "        # txc, w,h to t, x,c,w,h\n",
    "        tensor_list = [tensor.unsqueeze(0) for tensor in tensor_list]\n",
    "\n",
    "        # Concatena a lo largo de una nueva dimensión al principio, formando un tensor de (T, C, W, H)\n",
    "        combined_tensor = torch.cat(tensor_list, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return torch.cat(tensor_list), int(img_folder.split(\"/\")[-2])\n",
    "\n",
    "\n",
    "class FireDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=16, img_size=112, num_workers=12):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = FireSeriesDataset(\n",
    "            os.path.join(self.data_dir, \"train\"), self.img_size, is_train=True\n",
    "        )\n",
    "        self.val_dataset = FireSeriesDataset(\n",
    "            os.path.join(self.data_dir, \"val\"), self.img_size, is_train=False\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "class FireClassifier(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4):\n",
    "        super(FireClassifier, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Usamos una ResNet como extractor de características.\n",
    "        # Pretrained sobre ImageNet, usualmente se carga con 3 canales.\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Removemos la capa final para usarla como extractor de características.\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "\n",
    "        # LSTM que procesará las características extraídas.\n",
    "        # Número de características de la salida del último bloque conv de ResNet.\n",
    "        num_features = resnet.fc.in_features\n",
    "        self.lstm = nn.LSTM(input_size=32768, hidden_size=256, batch_first=True, num_layers=1)\n",
    "\n",
    "        # Capa de clasificación.\n",
    "        self.classifier = nn.Linear(256, 1)  # Salida binaria\n",
    "\n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # Métricas\n",
    "        self.train_accuracy = Accuracy(task=\"binary\")\n",
    "        self.val_accuracy = Accuracy(task=\"binary\")\n",
    "        self.train_precision = Precision(task=\"binary\")\n",
    "        self.val_precision = Precision(task=\"binary\")\n",
    "        self.train_recall = Recall(task=\"binary\")\n",
    "        self.val_recall = Recall(task=\"binary\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, channels, height, width]\n",
    "        # Procesa cada imagen de la secuencia a través del extractor de características.\n",
    "        batch_size, seq_length, C, H, W = x.size()\n",
    "        x = x.view(batch_size * seq_length, C, H, W)\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        # Reformatear salida para la LSTM\n",
    "        # Deberás asegurarte que las dimensiones coincidan con lo que espera la LSTM.\n",
    "        x = x.view(batch_size, seq_length, -1)\n",
    "\n",
    "        # Pasar las características por la LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "\n",
    "        # Solo nos interesa la última salida de la secuencia para la clasificación\n",
    "        x = x[:, -1, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_hat = self(x).squeeze()\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        acc = self.train_accuracy(torch.sigmoid(y_hat), y.int())\n",
    "        precision = self.train_precision(torch.sigmoid(y_hat), y.int())\n",
    "        recall = self.train_recall(torch.sigmoid(y_hat), y.int())\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        self.log(\"train_precision\", precision)\n",
    "        self.log(\"train_recall\", recall)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze()\n",
    "\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y.float())\n",
    "        acc = self.val_accuracy(torch.sigmoid(y_hat), y.int())\n",
    "        precision = self.val_precision(torch.sigmoid(y_hat), y.int())\n",
    "        recall = self.val_recall(torch.sigmoid(y_hat), y.int())\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_precision\", precision)\n",
    "        self.log(\"val_recall\", recall)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams['learning_rate'], weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': scheduler\n",
    "        }\n",
    "\n",
    "# Initialize the DataModule\n",
    "data_dir = \"/data/nisla/temporal_ds/images\"\n",
    "data_module = FireDataModule(data_dir)\n",
    "# set\n",
    "data_module.setup()\n",
    "# Función para obtener la imagen y etiqueta por índice de lote y posición\n",
    "def get_image_by_index(loader, batch_index, img_index):\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "# Extrae la imagen y etiqueta específicas\n",
    "image, label = get_image_by_index(train_loader, 1, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# setup data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data_module\u001b[38;5;241m.\u001b[39msetup()\n\u001b[1;32m----> 5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 163\u001b[0m, in \u001b[0;36mFireDataModule.train_dataloader\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\nico\\wildfire2024\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\nico\\wildfire2024\\.venv\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:144\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "data_dir = \"/data/nisla/temporal_ds/images\"\n",
    "data_module = FireDataModule(data_dir)\n",
    "data_module.setup()\n",
    "\n",
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener la imagen y etiqueta por índice de lote y posición\n",
    "def get_image_by_index(loader, batch_index, img_index):\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "\n",
    "\n",
    "# Extrae la imagen y etiqueta específicas\n",
    "image, label = get_image_by_index(train_loader, 1, 4) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
